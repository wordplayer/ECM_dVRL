{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arclab/vrep_dvrk_sim/dvrk_simulator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../../dvrk_simulator')\n",
    "path = os.getcwd()\n",
    "\n",
    "print(path)\n",
    "\n",
    "import dvrk_simulator.vrep.vrep as vrep\n",
    "import dvrk_simulator.vrep.ArmPSM as ArmPSM\n",
    "import dvrk_simulator.vrep.simObjects as simObjects\n",
    "import transforms3d\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clientID = vrep.simxStart(\"172.17.0.1\",19999,True,True,5000,5) # Connect to V-REP\n",
    "res = vrep.simxSynchronous(clientID , True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0.]), 0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vrep.simxStartSimulation(clientID, vrep.simx_opmode_blocking)\n",
    "vrep.simxSynchronous(clientID, True)\n",
    "vrep.simxSynchronousTrigger(clientID)\n",
    "vrep.simxGetPingTime(clientID)\n",
    "\n",
    "psm1 = ArmPSM.ArmPSM(armNumber=1, clientID=clientID)\n",
    "psm1.getPoseAtEE(ignoreError = True, initialize=True)\n",
    "psm1.getJointAngles(ignoreError=True, initialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get position\n",
      "1\n",
      "Failed to get orientation\n",
      "1\n",
      "Failed to get position\n",
      "1\n",
      "Failed to get orientation\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "left_cam = simObjects.camera(clientID=clientID, string_name=\"Vision_sensor_left\")\n",
    "right_cam = simObjects.camera(clientID=clientID, string_name=\"Vision_sensor_right\")\n",
    "\n",
    "def getStereoImagePairs():\n",
    "    img_left  = np.fliplr(left_cam.getImage())\n",
    "    img_right = np.fliplr(right_cam.getImage())\n",
    "    \n",
    "#     img_left[ np.where(img_left  == [0,0,0])] = 255\n",
    "#     img_right[np.where(img_right == [0,0,0])] = 255\n",
    "\n",
    "    return img_left, img_right\n",
    "\n",
    "left_cam.getPoseAtHandle( psm1.base_handle,  left_cam.camera_handle,  initialize=True)\n",
    "left_cam.getPoseAtHandle(left_cam.camera_handle, right_cam.camera_handle, initialize=True)\n",
    "\n",
    "vrep.simxSynchronousTrigger(clientID)\n",
    "vrep.simxGetPingTime(clientID)\n",
    "\n",
    "def getCurrentHandEye():\n",
    "    return left_cam.getPoseAtHandle( psm1.base_handle, left_cam.camera_handle,  initialize=False)\n",
    "\n",
    "def vrep_to_transforms_quat(quat):\n",
    "    return np.array([quat[3], quat[0], quat[1], quat[2]])\n",
    "\n",
    "\n",
    "def transforms_to_vrep_quat(quat):\n",
    "    return np.array([quat[1], quat[2], quat[3], quat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand eye is: \n",
      "tvec = [140.60425758 -68.75953078  -3.63826752]\n",
      "rvec = [ 0.43683098 -2.75821918  1.16153308]\n",
      "In matrix form hand-eye is:\n",
      "[[-9.51578403e-01 -3.07379641e-01  4.03731438e-03  1.40604258e+02]\n",
      " [-2.17668246e-01  6.64461846e-01 -7.14920268e-01 -6.87595308e+01]\n",
      " [ 2.17069294e-01 -6.81181482e-01 -6.99194329e-01 -3.63826752e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "inverse tvec is = [119.61896454  86.42865054 -52.26910179]\n",
      "inverse rvec is = [-0.43683098  2.75821918 -1.16153308]\n",
      "Left camera to right camera transform is:\n",
      "T = [5.00004739 0.24092197 0.72932243]\n",
      "R = [[ 1.00000000e+00  5.36441804e-07 -5.96046368e-08]\n",
      " [-5.36441802e-07  1.00000000e+00  2.98023384e-08]\n",
      " [ 5.96046528e-08 -2.98023064e-08  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "pos, quat = getCurrentHandEye()\n",
    "\n",
    "axis_angle = transforms3d.quaternions.quat2axangle(vrep_to_transforms_quat(quat))\n",
    "print(\"Hand eye is: \")\n",
    "print(\"tvec = {}\".format(pos*1000.0))\n",
    "print(\"rvec = {}\".format(axis_angle[0]*axis_angle[1]))\n",
    "\n",
    "print(\"In matrix form hand-eye is:\")\n",
    "R = transforms3d.axangles.axangle2mat(axis_angle[0], axis_angle[1])\n",
    "T = np.eye(4)\n",
    "T[:3, :3] = R\n",
    "T[:3, -1] = pos*1000.0\n",
    "\n",
    "print(\"{}\".format(T))\n",
    "a_t = transforms3d.axangles.mat2axangle(R.transpose())\n",
    "print(\"inverse tvec is = {}\".format(np.linalg.inv(T)[:3,-1]))\n",
    "print(\"inverse rvec is = {}\".format(a_t[0]*a_t[1]))\n",
    "\n",
    "\n",
    "pos, quat = left_cam.getPoseAtHandle(left_cam.camera_handle, right_cam.camera_handle, initialize=True)\n",
    "print(\"Left camera to right camera transform is:\")\n",
    "print(\"T = {}\".format(pos*1000.0))\n",
    "print(\"R = {}\".format(transforms3d.quaternions.quat2mat(vrep_to_transforms_quat(quat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get joint angle\n",
      "1\n",
      "Failed to get joint angle\n",
      "1\n",
      "Failed to get joint angle\n",
      "1\n",
      "Failed to get joint angle\n",
      "1\n",
      "Failed to get position\n",
      "1\n",
      "Failed to get orientation\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0.]), array([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dvrk_simulator.vrep.vrepObject import vrepObject\n",
    "\n",
    "class ArmECM(vrepObject):\n",
    "    def __init__(self, clientID):\n",
    "\n",
    "        super(ArmECM, self).__init__(clientID)\n",
    "        self.base_handle = self.getHandle('L0_respondable_ECM')\n",
    "        self.j1_handle   = self.getHandle('J1_ECM')\n",
    "        self.j2_handle   = self.getHandle('J2_ECM')\n",
    "        self.j3_handle   = self.getHandle('J3_ECM')\n",
    "        self.j4_handle   = self.getHandle('J4_ECM')\n",
    "        \n",
    "        self.left_sensor = self.getHandle('Vision_sensor_left')\n",
    "        \n",
    "    def getJointAngles(self, ignoreError = False, initialize = False):\n",
    "        pos1  = self.getJointPosition(self.j1_handle,  ignoreError, initialize)\n",
    "        pos2  = self.getJointPosition(self.j2_handle,  ignoreError, initialize)\n",
    "        pos3  = self.getJointPosition(self.j3_handle,  ignoreError, initialize)\n",
    "        pos4  = self.getJointPosition(self.j4_handle,  ignoreError, initialize)\n",
    "        \n",
    "        return np.array([pos1, pos2, pos3, pos4])\n",
    "    \n",
    "    def setJointAngles(self, jointAngles, ignoreError = False):\n",
    "\n",
    "        self.setJointPosition(self.j1_handle, jointAngles[0], ignoreError)\n",
    "        self.setJointPosition(self.j2_handle, jointAngles[1], ignoreError)\n",
    "        self.setJointPosition(self.j3_handle, jointAngles[2], ignoreError)\n",
    "        self.setJointPosition(self.j4_handle, jointAngles[3], ignoreError)\n",
    "        \n",
    "    def getCurrentPose(self, ignoreError = False, initialize = False):\n",
    "        return self.getPoseAtHandle(self.left_sensor, self.base_handle, ignoreError, initialize )\n",
    "    \n",
    "    \n",
    "ecm = ArmECM(clientID=clientID)\n",
    "ecm.getJointAngles(initialize=True)\n",
    "ecm.getCurrentPose(initialize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roslib\n",
    "import rospy\n",
    "import math\n",
    "from sensor_msgs.msg import Image\n",
    "from sensor_msgs.msg import JointState\n",
    "from geometry_msgs.msg import PoseStamped\n",
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "\n",
    "\n",
    "rospy.init_node('vrep_simulation', anonymous=True)\n",
    "l_image_pub = rospy.Publisher(\"/stereo/left/image\",Image, queue_size=1)\n",
    "r_image_pub = rospy.Publisher(\"/stereo/right/image\",Image, queue_size=1)\n",
    "j_publisher = rospy.Publisher(\"/vrep_sim/PSM1/joint_angles\", JointState, queue_size=1)\n",
    "j_ecm_publisher = rospy.Publisher(\"/vrep_sim/ECM/joint_angles\", JointState, queue_size=1)\n",
    "p_publisher = rospy.Publisher(\"/vrep_sim/ECM/position_cartesian_current\", PoseStamped, queue_size=1)\n",
    "\n",
    "bridge = CvBridge()\n",
    "\n",
    "\n",
    "haveNewJTrackedData = False\n",
    "haveNewPTrackedData = False\n",
    "haveNewLTrackedData = False\n",
    "haveNewRTrackedData = False\n",
    "\n",
    "most_recent_tracked_joint = None\n",
    "most_recent_tracked_pose = None\n",
    "most_recent_tracked_limg = None\n",
    "most_recent_tracked_rimg = None\n",
    "\n",
    "def j_tracked_callback(j_msg):\n",
    "    global most_recent_tracked_joint\n",
    "    most_recent_tracked_joint = j_msg.position\n",
    "    \n",
    "    global haveNewJTrackedData\n",
    "    haveNewJTrackedData = True\n",
    "\n",
    "\n",
    "def pose_tracked_callback(p_msg):\n",
    "    global most_recent_tracked_pose\n",
    "    most_recent_tracked_pose = p_msg.pose\n",
    "    \n",
    "    global haveNewPTrackedData\n",
    "    haveNewPTrackedData = True\n",
    "    \n",
    "def left_tracked_image_callback(l_msg):\n",
    "    global most_recent_tracked_limg\n",
    "    most_recent_tracked_limg = CvBridge().imgmsg_to_cv2(l_msg, desired_encoding='passthrough')\n",
    "    \n",
    "    global haveNewLTrackedData\n",
    "    haveNewLTrackedData = True\n",
    "    \n",
    "def right_tracked_image_callback(r_msg):\n",
    "    global most_recent_tracked_rimg\n",
    "    most_recent_tracked_rimg = CvBridge().imgmsg_to_cv2(r_msg, desired_encoding='passthrough')\n",
    "\n",
    "    global haveNewRTrackedData\n",
    "    haveNewRTrackedData = True\n",
    "\n",
    "\n",
    "\n",
    "j_tracked_sub     = rospy.Subscriber(\"/particle_filter/PSM1/state_joint_current\", JointState, j_tracked_callback)\n",
    "pose_tracked_sub  = rospy.Subscriber(\"/particle_filter/PSM1/position_cartesian_current\", PoseStamped, pose_tracked_callback)\n",
    "l_image_tracked_sub = rospy.Subscriber(\"/stereo/render/left/image\", Image, left_tracked_image_callback)\n",
    "r_image_tracked_sub = rospy.Subscriber(\"/stereo/render/right/image\", Image, right_tracked_image_callback)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "psm1.getJointAngles(ignoreError=True, initialize=True)\n",
    "\n",
    "\n",
    "def getStretchedJointAngles(cableStretchPer90, cableStretchInsertionPerCM, cableStretchJawAngle):\n",
    "    cableStretch_radians = cableStretchPer90*np.pi/180.0\n",
    "    cableStretchInsertion_meters = cableStretchInsertionPerCM/100.0\n",
    "\n",
    "    joint_angles, jaw_angle = psm1.getJointAngles(ignoreError=True, initialize=False)\n",
    "    \n",
    "    for i in range(0, len(joint_angles)):\n",
    "        if(i == 2):\n",
    "            joint_angles[i] = joint_angles[i]/(1.0 + cableStretchInsertion_meters/0.2 )\n",
    "        else:\n",
    "            joint_angles[i] = joint_angles[i]/(1.0 + cableStretch_radians/(np.pi/2.0))\n",
    "        \n",
    "    jaw_angle = jaw_angle/(1.0 + cableStretchJawAngle)\n",
    "    \n",
    "    return joint_angles, jaw_angle\n",
    "\n",
    "def applyNoisyJointAnglesECM(joint_angles, std_rot, std_prism):    \n",
    "    for i in range(0, len(joint_angles)):\n",
    "        if(i == 2):\n",
    "            joint_angles[i] += np.random.normal(scale=std_prism)\n",
    "        else:\n",
    "            joint_angles[i] += np.random.normal(scale=std_rot)\n",
    "            \n",
    "    return joint_angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomRotation(max_rotation_about_axis = 0.05):\n",
    "    # taken from second answer here: \n",
    "    # https://math.stackexchange.com/questions/442418/random-generation-of-rotation-matrices\n",
    "    \n",
    "    # \"Uniformly sample a random axis\" in the spherical space\n",
    "    theta  = np.arccos(np.random.uniform(low=-1,high=1))\n",
    "    phi    = np.random.uniform(low=0, high=2*np.pi)\n",
    "    vector = np.array([np.sin(phi)*np.cos(theta), np.sin(phi)*np.sin(theta), np.cos(phi)])\n",
    "    \n",
    "    # Uniformly sample angle to rotate about\n",
    "    angle  = np.random.uniform(low = 0, high = max_rotation_about_axis)\n",
    "    return transforms3d.axangles.axangle2mat(angle=angle, axis=vector)\n",
    "\n",
    "def applyRandomNoiseToPose(pos, quat, std_pos_noise, max_rotation_about_random_axis):\n",
    "    pos += np.random.normal(loc=0, scale=std_pos_noise, size=(3,))\n",
    "    \n",
    "    randomR  = randomRotation(max_rotation_about_random_axis)\n",
    "    currentR = np.dot(transforms3d.quaternions.quat2mat(vrep_to_transforms_quat(quat)), randomR)\n",
    "    quat =  transforms_to_vrep_quat(transforms3d.quaternions.mat2quat(currentR))\n",
    "\n",
    "    return pos, quat\n",
    "\n",
    "def posQuatToMatrix(pos, quat):\n",
    "    T = np.eye(4)\n",
    "    T[:3,:3] = transforms3d.quaternions.quat2mat(quat)\n",
    "    T[:3, 3] = pos\n",
    "    return T\n",
    "\n",
    "#Applies right-hand-side noise\n",
    "def applyNormalNoiseToPose(pos, quat, std_pos_noise, std_aangle_noise):\n",
    "    T_noise = np.eye(4)\n",
    "    \n",
    "    if(std_aangle_noise !=0):\n",
    "        rvec = np.random.normal(loc=0, scale= std_aangle_noise, size=(3,))\n",
    "        T_noise[:3,:3] = transforms3d.axangles.axangle2mat(rvec/np.linalg.norm(rvec), np.linalg.norm(rvec));\n",
    "        \n",
    "    T_noise[:3, 3] = np.random.normal(loc=0, scale=std_pos_noise, size=(3,))\n",
    "    T_out = np.dot(posQuatToMatrix(pos, quat), T_noise)\n",
    "    return T_out[:3,3], transforms3d.quaternions.mat2quat(T_out[:3,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeHandEye(pos, quat, pos_ecm, quat_ecm, filePath):\n",
    "    pos *=1000.0\n",
    "    axis_angle = transforms3d.quaternions.quat2axangle(vrep_to_transforms_quat(quat))\n",
    "    axis_angle = axis_angle[0]*axis_angle[1]\n",
    "    \n",
    "    \n",
    "    pos_ecm *= 1000.0\n",
    "    quat_ecm = vrep_to_transforms_quat(quat_ecm)\n",
    "    axis_angle_ecm = transforms3d.quaternions.quat2axangle(quat_ecm)\n",
    "    axis_angle_ecm = axis_angle_ecm[0]*axis_angle_ecm[1]\n",
    "    \n",
    "\n",
    "    with open(filePath, 'w+') as file:\n",
    "        file.write(\"%YAML:1.0\\n\")\n",
    "        file.write(\"---\\n\")\n",
    "        file.write(\"PSM1_tvec: [{}, {}, {}]\\n\".format(pos[0], pos[1], pos[2]))\n",
    "        file.write(\"PSM1_rvec: [{}, {}, {}]\\n\".format(axis_angle[0], axis_angle[1], axis_angle[2]))\n",
    "        \n",
    "        file.write(\"cam_tvec: [{}, {}, {}]\\n\".format(pos_ecm[0], pos_ecm[1], pos_ecm[2]))\n",
    "        file.write(\"cam_rvec: [{}, {}, {}]\\n\".format(axis_angle_ecm[0], axis_angle_ecm[1], axis_angle_ecm[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def createDirectory(folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import csv\n",
    "import tf\n",
    "\n",
    "\n",
    "e_pos, e_quat = psm1.getPoseAtHandle(psm1.EE_virtual_handle, left_cam.camera_handle,  initialize=True)\n",
    "\n",
    "\n",
    "def runMovingArmAndECMExperiment(cableStretchPer90, cableStretchInsertionPerCM, cableStretchJawAngle, \n",
    "                                 abs_max_bias_rot, abs_max_bias_ins, abs_max_bias_jaw, \n",
    "                                 std_rot_ecm, std_prism_ecm,\n",
    "                                 std_pos_noise, std_rot_noise, #for hand-eye noise\n",
    "                                 hand_eye_filepath, save_folder):\n",
    "    #save hand-eye\n",
    "    starting_ecm_j_angles = [0., 0.2617994,  0., 0.]\n",
    "    ecm.setJointAngles(starting_ecm_j_angles)\n",
    "\n",
    "    vrep.simxSynchronousTrigger(clientID)\n",
    "    vrep.simxGetPingTime(clientID)        \n",
    "    vrep.simxSynchronousTrigger(clientID)\n",
    "    vrep.simxGetPingTime(clientID)\n",
    "\n",
    "    h_pos, h_quat = getCurrentHandEye()\n",
    "    h_pos, h_quat = applyNormalNoiseToPose(h_pos, h_quat, std_pos_noise, std_rot_noise)\n",
    "    ecm_pos, ecm_quat = ecm.getCurrentPose()\n",
    "    writeHandEye(h_pos, h_quat, ecm_pos, ecm_quat, hand_eye_filepath)\n",
    "    \n",
    "    cmd = [\"rosrun\", \"particle_filter\", \"particle_filter_node\", \n",
    "           \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_configure_filter_with_ecm.json\"]\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    std_of_random_position_traj = 0.0001 #in m, this is additive noise on trajectory\n",
    "    max_rotation_about_random_axis_traj = 0.07 #in radians for trajectory, random walk!\n",
    "\n",
    "    rate = rospy.Rate(100)\n",
    "\n",
    "    #parameters for position trajectory\n",
    "    xy_circle_amplitude = 0.025\n",
    "    xy_circle_frequency = np.pi/180.0\n",
    "    z_circle_amplitude = 0.01\n",
    "    z_circle_frequency = np.pi/90.0\n",
    "    jaw_rate = 1/25.0\n",
    "\n",
    "    center_point  = [ 0.1405422 ,  0.03579695, -0.10973072];\n",
    "    starting_quat = [-0.47013299, -0.26924748, -0.14768405,  0.827448  ]\n",
    "    quat = starting_quat\n",
    "    \n",
    "    #Generate random bias values\n",
    "    j_bias = np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 2)\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_ins/100.0,abs_max_bias_ins/100.0,1))\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 3))\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_jaw, abs_max_bias_jaw, 1))\n",
    "    \n",
    "    j_ecm_bias = np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 2)\n",
    "    j_ecm_bias = np.append(j_ecm_bias, np.random.uniform(-abs_max_bias_ins/100.0,abs_max_bias_ins/100.0,1))\n",
    "    j_ecm_bias = np.append(j_ecm_bias, np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 1))\n",
    "\n",
    "    \n",
    "    #CSV saving files\n",
    "    createDirectory(save_folder)\n",
    "\n",
    "    realHandeye_file = open(save_folder + \"realHandeye.csv\", 'wb')\n",
    "    realHandeye_csv  = csv.writer(realHandeye_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    realEndEffector_file = open(save_folder + \"realEndEffector.csv\", 'wb')\n",
    "    realEndEffector_csv  = csv.writer(realEndEffector_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    realJointAngles_file = open(save_folder + \"realJointAngles.csv\", 'wb')\n",
    "    realJointAngles_csv  = csv.writer(realJointAngles_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    measuredJointAngles_file = open(save_folder + \"measuredJointAngles.csv\", 'wb')\n",
    "    measuredJointAngles_csv  = csv.writer(measuredJointAngles_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    trackedJointAngles_file = open(save_folder + \"trackedJointAngles.csv\", 'wb')\n",
    "    trackedJointAngles_csv  = csv.writer(trackedJointAngles_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    trackedEndEffector_file = open(save_folder + \"trackedEndEffector.csv\", 'wb')\n",
    "    trackedEndEffector_csv  = csv.writer(trackedEndEffector_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    trackedLumpedError_file = open(save_folder + \"trackedLumpedError.csv\", 'wb')\n",
    "    trackedLumpedError_csv  = csv.writer(trackedLumpedError_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    listener = tf.TransformListener()\n",
    "    \n",
    "    #ECM specific stuff!\n",
    "    realECMPose_file = open(save_folder + \"realECMPose.csv\", 'wb')\n",
    "    realECMPose_csv  = csv.writer(realECMPose_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    realJointAnglesECM_file = open(save_folder + \"realECMJointAngles.csv\", 'wb')\n",
    "    realJointAnglesECM_csv  = csv.writer(realJointAnglesECM_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    measuredJointAnglesECM_file = open(save_folder + \"measuredECMJointAngles.csv\", 'wb')\n",
    "    measuredJointAnglesECM_csv  = csv.writer(measuredJointAnglesECM_file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    \n",
    "    #Folders to save the images\n",
    "    rawImageFilePath = save_folder + \"rawImages/\"\n",
    "    createDirectory(rawImageFilePath)\n",
    "    \n",
    "    #Folders to save the images\n",
    "    renderImageFilePath = save_folder + \"renderImages/\"\n",
    "    createDirectory(renderImageFilePath)\n",
    "\n",
    "    \n",
    "    pose_header = [\"Time Stamp\", \"x\", \"y\", \"z\", \"q_w\", \"q_x\", \"q_y\", \"q_z\"]\n",
    "    realHandeye_csv.writerow(pose_header)\n",
    "    realEndEffector_csv.writerow(pose_header)\n",
    "    trackedEndEffector_csv.writerow(pose_header)\n",
    "    trackedLumpedError_csv.writerow(pose_header)\n",
    "    realECMPose_csv.writerow(pose_header)\n",
    "    \n",
    "    joint_header = [\"Time Stamp\", \"j1\", \"j2\", \"j3\", \"j4\", \"j5\", \"j6\", \"j7\"]\n",
    "    realJointAngles_csv.writerow(joint_header)\n",
    "    measuredJointAngles_csv.writerow(joint_header)\n",
    "    trackedJointAngles_csv.writerow(joint_header)\n",
    "    realJointAnglesECM_csv.writerow(joint_header)\n",
    "    measuredJointAnglesECM_csv.writerow(joint_header)\n",
    "                                             \n",
    "    start_itr = 0.0\n",
    "    loop_itr  = 0.0\n",
    "    \n",
    "    firstTime =  True\n",
    "    while not rospy.is_shutdown():\n",
    "\n",
    "        #Do the trajectory\n",
    "        x = xy_circle_amplitude*np.cos(xy_circle_frequency*loop_itr)\n",
    "        y = xy_circle_amplitude*np.sin(xy_circle_frequency*loop_itr)\n",
    "        z = z_circle_amplitude*np.sin(z_circle_frequency*loop_itr)\n",
    "\n",
    "        pos  = center_point + np.array([x, y, z])\n",
    "        pos, quat = applyRandomNoiseToPose(pos, quat, std_of_random_position_traj, max_rotation_about_random_axis_traj)\n",
    "\n",
    "        #Move jaw!\n",
    "        jaw  = math.fmod(jaw_rate*loop_itr, 2.0)\n",
    "        if jaw > 1.0:\n",
    "            jaw = 2.0 - jaw\n",
    "\n",
    "        psm1.setPoseAtEE(pos, quat, 1 - jaw)\n",
    "                                             \n",
    "        #Do the trajectory for the ecm\n",
    "\n",
    "        #first get the non-noisy pose which is published to the ros world later  \n",
    "        ecm_j_angles = starting_ecm_j_angles + np.array([0.2*np.sin(2.0*xy_circle_frequency*loop_itr),\n",
    "                                                         -0.1*np.sin(xy_circle_frequency*loop_itr),\n",
    "                                                         -0.05*np.sin(xy_circle_frequency*loop_itr),\n",
    "                                                         0.3*np.sin(xy_circle_frequency*loop_itr)])\n",
    "                                             \n",
    "        ecm.setJointAngles(ecm_j_angles)\n",
    "        vrep.simxSynchronousTrigger(clientID)\n",
    "        vrep.simxGetPingTime(clientID)        \n",
    "\n",
    "        pos, quat = ecm.getCurrentPose()\n",
    "        p_msg = PoseStamped()\n",
    "        p_msg.pose.position.x = pos[0]\n",
    "        p_msg.pose.position.y = pos[1]\n",
    "        p_msg.pose.position.z = pos[2]\n",
    "        p_msg.pose.orientation.x = quat[0]\n",
    "        p_msg.pose.orientation.y = quat[1]\n",
    "        p_msg.pose.orientation.z = quat[2]\n",
    "        p_msg.pose.orientation.w = quat[3]\n",
    "\n",
    "        #now set the noisy joint angles which give the image\n",
    "        ecm_noisy_j_angles = applyNoisyJointAnglesECM(ecm_j_angles, std_rot_ecm, std_prism_ecm) + j_ecm_bias\n",
    "        ecm.setJointAngles(ecm_noisy_j_angles)\n",
    "\n",
    "        vrep.simxSynchronousTrigger(clientID)\n",
    "        vrep.simxGetPingTime(clientID)   \n",
    "        \n",
    "                                                \n",
    "        global haveNewJTrackedData\n",
    "        global haveNewPTrackedData\n",
    "        global haveNewLTrackedData\n",
    "        global haveNewRTrackedData\n",
    "\n",
    "        haveNewJTrackedData = False\n",
    "        haveNewPTrackedData = False\n",
    "        haveNewLTrackedData = False\n",
    "        haveNewRTrackedData = False\n",
    "        \n",
    "\n",
    "        #Get timestamp and make the same for all data so it is sync'ed\n",
    "        timestamp = rospy.get_rostime()\n",
    "\n",
    "        #Publish images\n",
    "        l_img, r_img = getStereoImagePairs()\n",
    "        l_msg = bridge.cv2_to_imgmsg(l_img , encoding =\"rgb8\");\n",
    "        l_msg.header.stamp = timestamp\n",
    "        r_msg = bridge.cv2_to_imgmsg(r_img , encoding =\"rgb8\");\n",
    "        r_msg.header.stamp = timestamp\n",
    "\n",
    "        l_image_pub.publish(l_msg)\n",
    "        r_image_pub.publish(r_msg)\n",
    "\n",
    "        #Publish joint angles\n",
    "        joint_angles, jaw_angle = getStretchedJointAngles(cableStretchPer90, \n",
    "                                                          cableStretchInsertionPerCM,\n",
    "                                                          cableStretchJawAngle)\n",
    "        j_msg = JointState()\n",
    "        j_msg.header.stamp = timestamp\n",
    "        j_msg.position = np.append(joint_angles, [jaw_angle]) + j_bias\n",
    "        j_publisher.publish(j_msg)\n",
    "\n",
    "        #publish ecm pose\n",
    "        p_msg.header.stamp = timestamp\n",
    "        p_publisher.publish(p_msg)\n",
    "        j_ecm_publisher.publish()\n",
    "        j_msg_ecm = JointState()\n",
    "        j_msg_ecm.header.stamp = timestamp\n",
    "        j_msg_ecm.position = ecm_j_angles\n",
    "        j_ecm_publisher.publish(j_msg_ecm)\n",
    "        \n",
    "        if firstTime:\n",
    "            time.sleep(0.5)\n",
    "            firstTime = False\n",
    "            continue\n",
    "\n",
    "        #\n",
    "        # Save all the data\n",
    "        #\n",
    "\n",
    "        #real hand-eye (redundantly saved)\n",
    "        h_pos, h_quat = getCurrentHandEye()\n",
    "        save_data = np.append(timestamp, h_pos)\n",
    "        realHandeye_csv.writerow(np.append(save_data, vrep_to_transforms_quat(h_quat)).tolist())\n",
    "        \n",
    "        #real end-effector in cam frame\n",
    "        e_pos, e_quat = psm1.getPoseAtHandle(psm1.EE_virtual_handle, left_cam.camera_handle,  initialize=False)\n",
    "        save_data = np.append(timestamp, e_pos)\n",
    "        realEndEffector_csv.writerow(np.append(save_data, vrep_to_transforms_quat(e_quat)).tolist())\n",
    "        \n",
    "        #real joint angles:\n",
    "        joint_angles, jaw_angle = psm1.getJointAngles(ignoreError=True, initialize=False)\n",
    "        save_data = np.append(timestamp, joint_angles)\n",
    "        realJointAngles_csv.writerow(np.append(save_data, jaw_angle).tolist())\n",
    "        \n",
    "        #measured joint angles:\n",
    "        measuredJointAngles_csv.writerow(np.append(timestamp, j_msg.position).tolist())\n",
    "\n",
    "        #raw images\n",
    "        cv2.imwrite(rawImageFilePath + \"l_\" +  str(timestamp.to_nsec()) + \".png\", l_img)\n",
    "        cv2.imwrite(rawImageFilePath + \"r_\" +  str(timestamp.to_nsec()) + \".png\", r_img)\n",
    "                                             \n",
    "        #ECM pose\n",
    "        ecm_pos, ecm_quat = ecm.getCurrentPose()\n",
    "        save_data = np.append(timestamp, ecm_pos)\n",
    "        realECMPose_csv.writerow(np.append(save_data, ecm_quat).tolist())\n",
    "                                             \n",
    "        #ECM real joint angles\n",
    "        save_data = np.append(timestamp, ecm_noisy_j_angles)\n",
    "        realJointAnglesECM_csv.writerow(save_data.tolist())\n",
    "        \n",
    "        #ECM measured joint angles\n",
    "        save_data = np.append(timestamp, ecm_j_angles)\n",
    "        measuredJointAnglesECM_csv.writerow(save_data.tolist())\n",
    "\n",
    "        #save tracked joint angles\n",
    "        global most_recent_tracked_joint\n",
    "        while not haveNewJTrackedData:\n",
    "            continue\n",
    "        trackedJointAngles_csv.writerow(np.append(timestamp, most_recent_tracked_joint).tolist())\n",
    "               \n",
    "            \n",
    "        #save tracked pose\n",
    "        global most_recent_tracked_pose\n",
    "        while not haveNewPTrackedData:\n",
    "            continue\n",
    "        save_data = np.append(timestamp, [most_recent_tracked_pose.position.x,\n",
    "                                          most_recent_tracked_pose.position.y,\n",
    "                                          most_recent_tracked_pose.position.z])\n",
    "        save_data = np.append(save_data, [most_recent_tracked_pose.orientation.w, \n",
    "                                          most_recent_tracked_pose.orientation.x,\n",
    "                                          most_recent_tracked_pose.orientation.y,\n",
    "                                          most_recent_tracked_pose.orientation.z])\n",
    "        trackedEndEffector_csv.writerow(save_data.tolist())\n",
    "        \n",
    "        # Save lumped error:\n",
    "        listener.waitForTransform(\"/PSM1_base\", \"/cam0\", timestamp, rospy.Duration(4.0))\n",
    "        (trans, rot) = listener.lookupTransform(\"/PSM1_base\", \"/cam0\", rospy.Time(0))\n",
    "        save_data = np.append(timestamp, trans)\n",
    "        save_data = np.append(save_data, rot[-1])\n",
    "        save_data = np.append(save_data, rot[:3])\n",
    "        trackedLumpedError_csv.writerow(save_data.tolist())\n",
    "        \n",
    "         #save rendered images!!!\n",
    "        global most_recent_tracked_limg\n",
    "        while not haveNewLTrackedData:\n",
    "            continue\n",
    "        cv2.imwrite(renderImageFilePath + \"l_\" +  str(timestamp.to_nsec()) + \".png\", most_recent_tracked_limg)\n",
    "\n",
    "        global most_recent_tracked_rimg\n",
    "        while not haveNewRTrackedData:\n",
    "            continue\n",
    "        cv2.imwrite(renderImageFilePath + \"r_\" +  str(timestamp.to_nsec()) + \".png\", most_recent_tracked_rimg)\n",
    "\n",
    "\n",
    "        start_itr += 1\n",
    "        if start_itr <= 20:\n",
    "            continue\n",
    "\n",
    "        loop_itr += 1\n",
    "        if loop_itr >= 125:\n",
    "            break\n",
    "            \n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goal is to integrate ECM motion into particle filter:\n",
    "1) Try to modify the predict for lumped error? Think about it.... This would involve publishing the joint topic for ECM, giving it a configuration file, etc.\n",
    "2) write small initialization function to tune parameters for tracking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_rot_ecm   = 0.01\n",
    "std_prism_ecm = 0.0025\n",
    "\n",
    "biasRot = 0.25 #deg\n",
    "biasIns = 0.2 #cm\n",
    "biasJaw = 0.01 #0 to 1 = open\n",
    "\n",
    "cableStretchRot = 2#1 #deg per 90deg\n",
    "cableStretchIns = 0.5#0.5 #cm per 20cm\n",
    "cableStretchJaw = 0.05#0.05 #per 1 = open\n",
    "\n",
    "\n",
    "handEyePosNois_std =  0.005#0.005#0.0025#0.001 #0.0025 m\n",
    "handEyeRotNoise_std = 0.05#0.05# 0.02 radians\n",
    "\n",
    "\n",
    "\n",
    "hand_eye_fp = \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_noisy_handeye.yaml\"\n",
    "\n",
    "\n",
    "runMovingArmAndECMExperiment(cableStretchRot, cableStretchIns, cableStretchJaw, \n",
    "                             biasRot, biasIns, biasJaw, \n",
    "                             std_rot_ecm, std_prism_ecm,\n",
    "                             handEyePosNois_std, handEyeRotNoise_std, #for hand-eye noise\n",
    "                             hand_eye_fp, \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/test/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial: 0 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 1 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 2 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 3 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 4 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 5 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 6 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 7 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 8 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 9 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 10 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 11 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 12 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 13 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 14 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 15 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 16 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 17 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 18 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 19 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 20 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 21 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 22 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 23 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 24 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 25 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 26 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 27 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 28 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 29 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 30 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 31 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 32 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 33 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 34 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 35 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 36 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 37 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 38 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 39 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 40 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 41 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 42 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 43 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 44 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 45 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 46 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 47 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 48 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 49 for h1_j8_l0_c1_b1_n1\n",
      "Starting trial: 0 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 1 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 2 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 3 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 4 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 5 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 6 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 7 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 8 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 9 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 10 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 11 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 12 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 13 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 14 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 15 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 16 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 17 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 18 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 19 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 20 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 21 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 22 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 23 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 24 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 25 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 26 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 27 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 28 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 29 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 30 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 31 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 32 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 33 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 34 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 35 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 36 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 37 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 38 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 39 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 40 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 41 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 42 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 43 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 44 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 45 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 46 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 47 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 48 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 49 for h1_j8_l0_c1_b1_n0\n",
      "Starting trial: 0 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 1 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 2 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 3 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 4 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 5 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 6 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 7 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 8 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 9 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 10 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 11 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 12 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 13 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 14 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 15 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 16 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 17 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 18 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 19 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 20 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 21 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 22 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 23 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 24 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 25 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 26 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 27 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 28 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 29 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 30 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 31 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 32 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 33 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 34 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 35 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 36 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 37 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 38 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 39 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 40 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 41 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 42 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 43 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 44 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 45 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 46 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 47 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 48 for h1_j8_l0_c0_b0_n1\n",
      "Starting trial: 49 for h1_j8_l0_c0_b0_n1\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "hand_eye_fp = \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_noisy_handeye.yaml\"\n",
    "\n",
    "std_rot_ecm   = [0, 0.01]\n",
    "std_prism_ecm = [0, 0.0025]\n",
    "\n",
    "cableStretchRot = [0, 2]#1 #deg per 90deg\n",
    "cableStretchIns = [0, 0.5]#0.5 #cm per 20cm\n",
    "cableStretchJaw = [0, 0.05]#0.05 #per 1 = open\n",
    "\n",
    "biasRot = [0, 0.25]#0.25 #deg\n",
    "biasIns = [0, 0.2]#0.1 #cm\n",
    "biasJaw = [0, 0.01]#0.01 #0 to 1 = open\n",
    "\n",
    "handEyePosNois_std =  [0, 0.005]#0.0025#0.001 #0.0025 m\n",
    "handEyeRotNoise_std = [0, 0.05]# 0.02 radians\n",
    "\n",
    "num_repeat_trials = 50\n",
    "\n",
    "\n",
    "trials_to_run = [ (1,1, 0), (0,0,1), (1,1,1)]\n",
    "\n",
    "# Naming convenction:\n",
    "# --Tracking specific--\n",
    "# h0, h1 = \"hyand-eye\" tracking off/on\n",
    "# j1, j2, ... , j8 = first joint to track\n",
    "# l0, l1 = lumped joint error prediction off/on\n",
    "# --Environment specific--\n",
    "# c0, c1 = cable stretch of/on\n",
    "# b0, b1 = bias off/on\n",
    "# n0, n1 = hand-eye noise off/on\n",
    "\n",
    "#Total number of combinations \n",
    "\n",
    "name_convention = \"h1_j8_l0\"\n",
    "\n",
    "save_parent_folder = \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/save_data_moving/\" + name_convention + \"/\"\n",
    "createDirectory(save_parent_folder)\n",
    "\n",
    "copyfile(\"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_configure_filter_with_ecm.json\",\n",
    "         save_parent_folder + \"vrep_dataset_configure_filter_with_ecm.json\")\n",
    "\n",
    "for i in reversed(range(len(cableStretchRot))):\n",
    "    for j in reversed(range(len(biasRot))):\n",
    "        for k in reversed(range(len(handEyePosNois_std))):\n",
    "            \n",
    "            if not ((i,j,k) in trials_to_run):\n",
    "                continue\n",
    "            \n",
    "            name = name_convention + \"_c{}_b{}_n{}\".format(i,j,k)\n",
    "            save_child_folder = save_parent_folder + name + \"/\"\n",
    "            createDirectory(save_child_folder)\n",
    "\n",
    "            for trial in range(num_repeat_trials):\n",
    "                #Skip if it already is there!\n",
    "                save_folder = save_child_folder + str(trial) + \"/\"\n",
    "                if os.path.exists(save_folder):\n",
    "                    continue\n",
    "\n",
    "                print(\"Starting trial: {} for {}\".format(trial, name))\n",
    "                \n",
    "                runMovingArmAndECMExperiment(cableStretchRot[i], cableStretchIns[i], cableStretchJaw[i], \n",
    "                             biasRot[j], biasIns[j], biasJaw[j], \n",
    "                             std_rot_ecm[i], std_prism_ecm[i],\n",
    "                             handEyePosNois_std[k], handEyeRotNoise_std[k], #for hand-eye noise\n",
    "                             hand_eye_fp, save_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import csv\n",
    "import tf\n",
    "\n",
    "\n",
    "e_pos, e_quat = psm1.getPoseAtHandle(psm1.EE_virtual_handle, left_cam.camera_handle,  initialize=True)\n",
    "\n",
    "\n",
    "def runTestInitializationArmAndECMExperiment(cableStretchPer90, cableStretchInsertionPerCM, cableStretchJawAngle, \n",
    "                                             abs_max_bias_rot, abs_max_bias_ins, abs_max_bias_jaw, \n",
    "                                             std_rot_ecm, std_prism_ecm,\n",
    "                                             std_pos_noise, std_rot_noise, #for hand-eye noise\n",
    "                                             hand_eye_filepath):\n",
    "    #save hand-eye\n",
    "    starting_ecm_j_angles = [0., 0.2617994,  0., 0.]\n",
    "    ecm.setJointAngles(starting_ecm_j_angles)\n",
    "\n",
    "    vrep.simxSynchronousTrigger(clientID)\n",
    "    vrep.simxGetPingTime(clientID)        \n",
    "    vrep.simxSynchronousTrigger(clientID)\n",
    "    vrep.simxGetPingTime(clientID)\n",
    "\n",
    "    h_pos, h_quat = getCurrentHandEye()\n",
    "    h_pos, h_quat = applyNormalNoiseToPose(h_pos, h_quat, std_pos_noise, std_rot_noise)\n",
    "    ecm_pos, ecm_quat = ecm.getCurrentPose()\n",
    "    writeHandEye(h_pos, h_quat, ecm_pos, ecm_quat, hand_eye_filepath)\n",
    "    \n",
    "    cmd = [\"rosrun\", \"particle_filter\", \"particle_filter_node\", \n",
    "           \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_configure_filter_with_ecm.json\"]\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(4)\n",
    "    \n",
    "    std_of_random_position_traj = 0.0001 #in m, this is additive noise on trajectory\n",
    "    max_rotation_about_random_axis_traj = 0.07 #in radians for trajectory, random walk!\n",
    "\n",
    "    rate = rospy.Rate(100)\n",
    "\n",
    "    #parameters for position trajectory\n",
    "    xy_circle_amplitude = 0.025\n",
    "    xy_circle_frequency = np.pi/180.0\n",
    "    z_circle_amplitude = 0.01\n",
    "    z_circle_frequency = np.pi/90.0\n",
    "    jaw_rate = 1/25.0\n",
    "\n",
    "    center_point  = [ 0.1405422 ,  0.03579695, -0.10973072];\n",
    "    starting_quat = [-0.47013299, -0.26924748, -0.14768405,  0.827448  ]\n",
    "    quat = starting_quat\n",
    "    \n",
    "    #Generate random bias values\n",
    "    j_bias = np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 2)\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_ins/100.0,abs_max_bias_ins/100.0,1))\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 3))\n",
    "    j_bias = np.append(j_bias, np.random.uniform(-abs_max_bias_jaw, abs_max_bias_jaw, 1))\n",
    "    \n",
    "    j_ecm_bias = np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 2)\n",
    "    j_ecm_bias = np.append(j_ecm_bias, np.random.uniform(-abs_max_bias_ins/100.0,abs_max_bias_ins/100.0,1))\n",
    "    j_ecm_bias = np.append(j_ecm_bias, np.random.uniform(-abs_max_bias_rot*np.pi/180.0, abs_max_bias_rot*np.pi/180.0, 1))\n",
    "                                             \n",
    "    start_itr = 0.0\n",
    "    loop_itr  = 0.0\n",
    "    \n",
    "    firstTime =  True\n",
    "    while not rospy.is_shutdown():\n",
    "\n",
    "        #Do the trajectory\n",
    "        x = xy_circle_amplitude*np.cos(xy_circle_frequency*loop_itr)\n",
    "        y = xy_circle_amplitude*np.sin(xy_circle_frequency*loop_itr)\n",
    "        z = z_circle_amplitude*np.sin(z_circle_frequency*loop_itr)\n",
    "\n",
    "        pos  = center_point + np.array([x, y, z])\n",
    "        pos, quat = applyRandomNoiseToPose(pos, quat, std_of_random_position_traj, max_rotation_about_random_axis_traj)\n",
    "\n",
    "        #Move jaw!\n",
    "        jaw  = math.fmod(jaw_rate*loop_itr, 2.0)\n",
    "        if jaw > 1.0:\n",
    "            jaw = 2.0 - jaw\n",
    "\n",
    "        psm1.setPoseAtEE(pos, quat, 1 - jaw)\n",
    "                                             \n",
    "        #Do the trajectory for the ecm\n",
    "\n",
    "        #first get the non-noisy pose which is published to the ros world later  \n",
    "        ecm_j_angles = starting_ecm_j_angles + np.array([0.2*np.sin(1.5*xy_circle_frequency*loop_itr),\n",
    "                                                         -0.1*np.sin(xy_circle_frequency*loop_itr),\n",
    "                                                         -0.05*np.sin(xy_circle_frequency*loop_itr),\n",
    "                                                         0.3*np.sin(xy_circle_frequency*loop_itr)])\n",
    "                                             \n",
    "        ecm.setJointAngles(ecm_j_angles)\n",
    "        vrep.simxSynchronousTrigger(clientID)\n",
    "        vrep.simxGetPingTime(clientID)        \n",
    "\n",
    "        pos, quat = ecm.getCurrentPose()\n",
    "        p_msg = PoseStamped()\n",
    "        p_msg.pose.position.x = pos[0]\n",
    "        p_msg.pose.position.y = pos[1]\n",
    "        p_msg.pose.position.z = pos[2]\n",
    "        p_msg.pose.orientation.x = quat[0]\n",
    "        p_msg.pose.orientation.y = quat[1]\n",
    "        p_msg.pose.orientation.z = quat[2]\n",
    "        p_msg.pose.orientation.w = quat[3]\n",
    "\n",
    "        #now set the noisy joint angles which give the image\n",
    "        ecm_noisy_j_angles = applyNoisyJointAnglesECM(ecm_j_angles, std_rot_ecm, std_prism_ecm) + j_ecm_bias\n",
    "        ecm.setJointAngles(ecm_noisy_j_angles)\n",
    "        \n",
    "        global haveNewPTrackedData\n",
    "        haveNewPTrackedData = False\n",
    "        \n",
    "        vrep.simxSynchronousTrigger(clientID)\n",
    "        vrep.simxGetPingTime(clientID)   \n",
    "        \n",
    "        #Get timestamp and make the same for all data so it is sync'ed\n",
    "        timestamp = rospy.get_rostime()\n",
    "\n",
    "        #Publish images\n",
    "        l_img, r_img = getStereoImagePairs()\n",
    "        l_msg = bridge.cv2_to_imgmsg(l_img , encoding =\"rgb8\");\n",
    "        l_msg.header.stamp = timestamp\n",
    "        r_msg = bridge.cv2_to_imgmsg(r_img , encoding =\"rgb8\");\n",
    "        r_msg.header.stamp = timestamp\n",
    "\n",
    "        l_image_pub.publish(l_msg)\n",
    "        r_image_pub.publish(r_msg)\n",
    "\n",
    "        #Publish joint angles\n",
    "        joint_angles, jaw_angle = getStretchedJointAngles(cableStretchPer90, \n",
    "                                                          cableStretchInsertionPerCM,\n",
    "                                                          cableStretchJawAngle)\n",
    "        j_msg = JointState()\n",
    "        j_msg.header.stamp = timestamp\n",
    "        j_msg.position = np.append(joint_angles, [jaw_angle]) + j_bias\n",
    "        j_publisher.publish(j_msg)\n",
    "\n",
    "        #publish ecm pose\n",
    "        p_msg.header.stamp = timestamp\n",
    "        p_publisher.publish(p_msg)\n",
    "        j_ecm_publisher.publish()\n",
    "        j_msg_ecm = JointState()\n",
    "        j_msg_ecm.header.stamp = timestamp\n",
    "        j_msg_ecm.position = ecm_j_angles\n",
    "        j_ecm_publisher.publish(j_msg_ecm)\n",
    "        \n",
    "        if firstTime:\n",
    "            time.sleep(0.5)\n",
    "            firstTime = False\n",
    "            continue\n",
    "            \n",
    "        #real end-effector in cam frame\n",
    "        e_pos, e_quat = psm1.getPoseAtHandle(psm1.EE_virtual_handle, left_cam.camera_handle,  initialize=False)                       \n",
    "        e_quat = vrep_to_transforms_quat(e_quat)    \n",
    "        T_extra = np.array([[-1, 0, 0, 0],\n",
    "                            [0, 1, 0, 0],\n",
    "                            [0, 0,  -1, 0],\n",
    "                            [0,  0,  0, 1]])\n",
    "        \n",
    "        T_real = np.dot(posQuatToMatrix(e_pos, e_quat), T_extra)        \n",
    "        e_pos = T_real[:3, 3]\n",
    "        \n",
    "        # tracked pose\n",
    "        global most_recent_tracked_pose\n",
    "        while not haveNewPTrackedData:\n",
    "            continue\n",
    "            \n",
    "        t_pos  = np.array([most_recent_tracked_pose.position.x, \n",
    "                           most_recent_tracked_pose.position.y, \n",
    "                           most_recent_tracked_pose.position.z])\n",
    "        \n",
    "        t_quat = np.array([most_recent_tracked_pose.orientation.w, \n",
    "                           most_recent_tracked_pose.orientation.x,\n",
    "                           most_recent_tracked_pose.orientation.y,\n",
    "                           most_recent_tracked_pose.orientation.z])\n",
    "        dist_error = np.linalg.norm(t_pos - e_pos)\n",
    "        \n",
    "        e_euler = transforms3d.euler.mat2euler(T_real[:3,:3])\n",
    "        ori_error  = np.linalg.norm(np.array(e_euler) - np.array(transforms3d.euler.quat2euler(t_quat)))\n",
    "\n",
    "\n",
    "        start_itr += 1\n",
    "        if start_itr >= 20:\n",
    "            break\n",
    "            \n",
    "    p.terminate()\n",
    "    return dist_error, ori_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Error d = 2.96965407949, Error o = 5.31471030031\n",
      "1 Error d = 13.511952787, Error o = 3.16896011978\n",
      "2 Error d = 0.496735876513, Error o = 5.4242439557\n",
      "3 Error d = 4.72511035371, Error o = 2.82230419342\n",
      "4 Error d = 1.17868829654, Error o = 3.07055537421\n",
      "5 Error d = 0.821303458618, Error o = 4.40428904455\n",
      "6 Error d = 5.2383870016, Error o = 6.03270983653\n",
      "7 Error d = 0.839768944185, Error o = 1.84217537988\n",
      "8 Error d = 4.48790487607, Error o = 3.5132000972\n",
      "9 Error d = 3.91145914832, Error o = 4.56969517437\n",
      "10 Error d = 0.818197314379, Error o = 2.53181858149\n",
      "11 Error d = 6.12687947602, Error o = 4.42985859553\n",
      "12 Error d = 6.05045380489, Error o = 3.88840810976\n",
      "13 Error d = 4.84665841518, Error o = 0.837226993037\n",
      "14 Error d = 1.1984549522, Error o = 3.76944822095\n",
      "15 Error d = 2.53241948982, Error o = 5.26001742323\n",
      "16 Error d = 6.4068503225, Error o = 3.9388989755\n",
      "17 Error d = 3.70696713771, Error o = 3.95367582489\n",
      "18 Error d = 2.84480304913, Error o = 6.73708665985\n",
      "19 Error d = 8.197370354, Error o = 9.74314339809\n",
      "20 Error d = 0.8650742497, Error o = 3.78521336227\n",
      "21 Error d = 3.87641691587, Error o = 4.68052975692\n",
      "22 Error d = 2.85744750174, Error o = 3.73499375724\n",
      "23 Error d = 0.882153271342, Error o = 6.10112052953\n",
      "24 Error d = 7.51687652263, Error o = 8.72239594231\n",
      "25 Error d = 5.89741559093, Error o = 9.71967613197\n",
      "26 Error d = 4.07428672879, Error o = 6.18544200419\n",
      "27 Error d = 5.12154526074, Error o = 3.87612015026\n",
      "28 Error d = 2.3263040836, Error o = 4.36534872887\n",
      "29 Error d = 2.93523900616, Error o = 5.40515319125\n",
      "30 Error d = 4.6351835782, Error o = 4.52014586275\n",
      "31 Error d = 2.11217220951, Error o = 3.16243760378\n",
      "32 Error d = 2.13417260696, Error o = 8.1751685691\n",
      "33 Error d = 3.59701033091, Error o = 9.89759167135\n",
      "34 Error d = 1.9237067641, Error o = 2.31085169693\n",
      "35 Error d = 4.32556451908, Error o = 5.38800547602\n",
      "36 Error d = 2.66239877749, Error o = 6.62924510391\n",
      "37 Error d = 12.3024683782, Error o = 5.89781628227\n",
      "38 Error d = 3.65139603178, Error o = 3.44243245183\n",
      "39 Error d = 4.90381453542, Error o = 3.77903134739\n",
      "40 Error d = 1.69765058515, Error o = 1.73608770972\n",
      "41 Error d = 0.540510251594, Error o = 3.26176054488\n",
      "42 Error d = 2.50010183292, Error o = 1.6671796153\n",
      "43 Error d = 0.832539598603, Error o = 2.88481611433\n",
      "44 Error d = 9.47771577712, Error o = 5.42635425275\n",
      "45 Error d = 7.11999474604, Error o = 9.42897313298\n",
      "46 Error d = 3.22744635948, Error o = 6.93288785126\n",
      "47 Error d = 3.45422783781, Error o = 4.94781428455\n",
      "48 Error d = 1.64875637376, Error o = 1.63229484859\n",
      "49 Error d = 2.54592561089, Error o = 6.99165023684\n",
      "Distance error  : 3.85111069949 +/- 2.79647418169 with max 13.511952787\n",
      "Orienation error: 4.79881928939 +/- 2.20205980423 with max 9.89759167135\n"
     ]
    }
   ],
   "source": [
    "hand_eye_fp = \"/home/arclab/vrep_dvrk_sim/dvrk_simulator/vrep_sims_for_tool_tracking/vrep_dataset_noisy_handeye.yaml\"\n",
    "\n",
    "std_rot_ecm   = 0.01\n",
    "std_prism_ecm = 0.0015\n",
    "    \n",
    "\n",
    "cableStretchRot = 1#1 #deg per 90deg\n",
    "cableStretchIns = 0.5#0.5#0.5 #cm per 20cm\n",
    "cableStretchJaw = 0.05#0.05 #per 1 = open\n",
    "\n",
    "biasRot = 0.25 #deg\n",
    "biasIns = 0.2#0.2 #cm\n",
    "biasJaw = 0.01 #0 to 1 = open\n",
    "\n",
    "handEyePosNois_std = 0.005 # mm\n",
    "handEyeRotNoise_std = 0.05 # 0.02 radians\n",
    "\n",
    "num_itr = 50\n",
    "\n",
    "d_error = np.zeros(num_itr)\n",
    "o_error = np.zeros(num_itr)\n",
    "\n",
    "for i in range(num_itr):\n",
    "    d_error[i], o_error[i] = runTestInitializationArmAndECMExperiment(cableStretchRot, cableStretchIns, cableStretchJaw, \n",
    "                                                                     biasRot, biasIns, biasJaw, \n",
    "                                                                     std_rot_ecm, std_prism_ecm,\n",
    "                                                                     handEyePosNois_std, handEyeRotNoise_std, \n",
    "                                                                     hand_eye_fp)\n",
    "    d_error[i] *= 1000.0\n",
    "    o_error[i] *= 180.0/np.pi\n",
    "    print(\"{} Error d = {}, Error o = {}\".format(i, d_error[i], o_error[i]))\n",
    "\n",
    "print(\"Distance error  : {} +/- {} with max {}\".format(d_error[:i+1].mean(), d_error[:i+1].std(), d_error[:i+1].max()))\n",
    "print(\"Orienation error: {} +/- {} with max {}\".format(o_error[:i+1].mean(), o_error[:i+1].std(), o_error[:i+1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Distance error  : 3.69363573165 +/- 1.5296423293 with max 6.64174671029\n",
    "Orienation error: 2.9765091081 +/- 0.691636358242 with max 4.01297214697\n",
    "\n",
    "No lumped error with 5, 1.0, 0.01\n",
    "Distance error  : 2.53235495209 +/- 1.58618594081 with max 5.809590102\n",
    "Orienation error: 3.3296414051 +/- 1.2038611327 with max 5.59308444028\n",
    "\n",
    "\n",
    "\n",
    "Lumped error calc with 5, 2.0, 0.02 for scale, sigma_t, and sigma_r\n",
    "\n",
    "\n",
    "Lumped error calc with 5, 2.0, 0.01 for scale, sigma_t, and sigma_r\n",
    "Distance error  : 7.34036114649 +/- 5.12099443903 with max 19.8896239718\n",
    "Orienation error: 4.69915720534 +/- 1.25253041033 with max 6.89017052801\n",
    "\n",
    "Lumped error calc with 20, 0.5, 0.01 for scale, sigma_t, and sigma_r\n",
    "Distance error  : 6.30505059259 +/- 5.2249912033 with max 15.4528597818\n",
    "Orienation error: 8.3714336771 +/- 5.09356951588 with max 16.2614062796\n",
    "\n",
    "This is tracking only lumped error with 20, 0.5, 0.01 for scale, sigma_t, and sigma_r\n",
    "Distance error  : 7.7232472287 +/- 7.9995704229 with max 26.9679491869\n",
    "Orienation error: 7.24919995577 +/- 3.20047095002 with max 12.136384228"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
